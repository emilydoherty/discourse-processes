{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN values:\n",
      "       Speaker            Start              End  \\\n",
      "0        301.0  01:06:56.520000  01:06:57.700000   \n",
      "1        299.0  01:06:57.770000  01:07:01.140000   \n",
      "2        300.0  01:06:58.805000  01:07:00.115000   \n",
      "3        299.0  01:07:01.710000  01:07:03.740000   \n",
      "4        301.0  01:07:02.435000  01:07:04.175000   \n",
      "...        ...              ...              ...   \n",
      "53532    260.0  00:51:06.850000  00:51:08.610000   \n",
      "53533    259.0  00:51:07.060000  00:51:08.620000   \n",
      "53534    258.0  00:51:13.940000  00:51:16.870000   \n",
      "53535    260.0  00:51:16.930000  00:51:31.080000   \n",
      "53536    258.0  00:51:36.060000  00:51:38.340000   \n",
      "\n",
      "                                                    Text Stop Transcript  \\\n",
      "0                                    what's the tutorial  NaN        NaN   \n",
      "1                                      I think this is A  NaN        NaN   \n",
      "2                                    show sound tutorial  NaN        NaN   \n",
      "3                                      that's B, this is  NaN        NaN   \n",
      "4                                 is it this right here?  NaN        NaN   \n",
      "...                                                  ...  ...        ...   \n",
      "53532           Then we should see just the sound under   NaN        NaN   \n",
      "53533                          can we download it again   NaN        NaN   \n",
      "53534  Oh. sure they're not sound intensity. Mm no. U...  NaN        NaN   \n",
      "53535                     Yeah it's working it's working  NaN        NaN   \n",
      "53536  It wasn't the sound intensity we fixed the har...  NaN        NaN   \n",
      "\n",
      "       speaker time_start time_end utterance  ...  \\\n",
      "0          NaN        NaN      NaN       NaN  ...   \n",
      "1          NaN        NaN      NaN       NaN  ...   \n",
      "2          NaN        NaN      NaN       NaN  ...   \n",
      "3          NaN        NaN      NaN       NaN  ...   \n",
      "4          NaN        NaN      NaN       NaN  ...   \n",
      "...        ...        ...      ...       ...  ...   \n",
      "53532      NaN        NaN      NaN       NaN  ...   \n",
      "53533      NaN        NaN      NaN       NaN  ...   \n",
      "53534      NaN        NaN      NaN       NaN  ...   \n",
      "53535      NaN        NaN      NaN       NaN  ...   \n",
      "53536      NaN        NaN      NaN       NaN  ...   \n",
      "\n",
      "       CPS_NEG_MonitorsE_Strategizes  CPS_NEG_MonitorsE_Save  \\\n",
      "0                                NaN                     NaN   \n",
      "1                                NaN                     NaN   \n",
      "2                                NaN                     NaN   \n",
      "3                                NaN                     NaN   \n",
      "4                                NaN                     NaN   \n",
      "...                              ...                     ...   \n",
      "53532                            NaN                     NaN   \n",
      "53533                            NaN                     NaN   \n",
      "53534                            NaN                     NaN   \n",
      "53535                            NaN                     NaN   \n",
      "53536                            NaN                     NaN   \n",
      "\n",
      "       CPS_MAINTAIN_Initiative_Suggestions  \\\n",
      "0                                      NaN   \n",
      "1                                      NaN   \n",
      "2                                      NaN   \n",
      "3                                      NaN   \n",
      "4                                      NaN   \n",
      "...                                    ...   \n",
      "53532                                  NaN   \n",
      "53533                                  NaN   \n",
      "53534                                  NaN   \n",
      "53535                                  NaN   \n",
      "53536                                  NaN   \n",
      "\n",
      "       CPS_MAINTAIN_Initiative_Compliments  \\\n",
      "0                                      NaN   \n",
      "1                                      NaN   \n",
      "2                                      NaN   \n",
      "3                                      NaN   \n",
      "4                                      NaN   \n",
      "...                                    ...   \n",
      "53532                                  NaN   \n",
      "53533                                  NaN   \n",
      "53534                                  NaN   \n",
      "53535                                  NaN   \n",
      "53536                                  NaN   \n",
      "\n",
      "       CPS_MAINTAIN_FulfillsR_InitiatesOffTopic  \\\n",
      "0                                           NaN   \n",
      "1                                           NaN   \n",
      "2                                           NaN   \n",
      "3                                           NaN   \n",
      "4                                           NaN   \n",
      "...                                         ...   \n",
      "53532                                       NaN   \n",
      "53533                                       NaN   \n",
      "53534                                       NaN   \n",
      "53535                                       NaN   \n",
      "53536                                       NaN   \n",
      "\n",
      "       CPS_MAINTAIN_FulfillsR_JoinsOffTopic  CPS_MAINTAIN_FulfillsR_Support  \\\n",
      "0                                       NaN                             NaN   \n",
      "1                                       NaN                             NaN   \n",
      "2                                       NaN                             NaN   \n",
      "3                                       NaN                             NaN   \n",
      "4                                       NaN                             NaN   \n",
      "...                                     ...                             ...   \n",
      "53532                                   NaN                             NaN   \n",
      "53533                                   NaN                             NaN   \n",
      "53534                                   NaN                             NaN   \n",
      "53535                                   NaN                             NaN   \n",
      "53536                                   NaN                             NaN   \n",
      "\n",
      "       CPS_MAINTAIN_FulfillsR_Apologizes  Notes  Unnamed: 4  \n",
      "0                                    NaN    NaN         NaN  \n",
      "1                                    NaN    NaN         NaN  \n",
      "2                                    NaN    NaN         NaN  \n",
      "3                                    NaN    NaN         NaN  \n",
      "4                                    NaN    NaN         NaN  \n",
      "...                                  ...    ...         ...  \n",
      "53532                                NaN    NaN         NaN  \n",
      "53533                                NaN    NaN         NaN  \n",
      "53534                                NaN    NaN         NaN  \n",
      "53535                                NaN    NaN         NaN  \n",
      "53536                                NaN    NaN         NaN  \n",
      "\n",
      "[53537 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nan_values = combined_df.isna()\n",
    "\n",
    "# Use any() to check if there are any NaN values in each column\n",
    "columns_with_nan = nan_values.any()\n",
    "# Display rows with NaN values\n",
    "print(\"Rows with NaN values:\")\n",
    "print(combined_df[combined_df.isna().any(axis=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Master files \n",
    "#Run for Makecode\n",
    "folder_path= '/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Cleaned Transcripts/Makecode/Clean'\n",
    "# folder_path='/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Cleaned Transcripts/Weights/Clean'\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.xlsx'):  # Adjust the file extension if needed\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Read each Excel file into a DataFrame and append it to the list\n",
    "        dfx = pd.read_excel(file_path)\n",
    "        dfs.append(dfx)\n",
    "\n",
    "# Concatenate all DataFrames in the list into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new Excel file\n",
    "combined_excel_path = '/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Cleaned Transcripts/Makecode/Clean/makecode_master.xlsx'  \n",
    "# combined_excel_path = '/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Cleaned Transcripts/Weights/Clean/weights_master.xlsx'  \n",
    "# Specify the desired output file path\n",
    "combined_df.to_excel(combined_excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create an output folder \n",
    "output_folder = '/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Other Measures/Plots/bargraph'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load the spreadsheet into a DataFrame\n",
    "df = pd.read_excel('/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Other Measures/AICLsurvey_long.xlsx', sheet_name='Sheet1')  # Replace 'Sheet1' with your actual sheet name if different\n",
    "df = df[df['Round'].isin([2, 4])]\n",
    "df['Round'] = df['Round'].replace({2: 'weights', 4: 'makecode'})\n",
    "\n",
    "# Group by Session ID and Round, and calculate the mean of survey measures\n",
    "grouped_data = df.groupby(['Session_ID', 'Round']).mean().reset_index()\n",
    "# List of survey measures\n",
    "survey_measures = ['TLXMentalDemand', 'TLXRush', 'TLXSuccessRC', 'SAMPleasure', \n",
    "                   'SAMArousal', 'SocialLoafAvg', 'SLPosIntAvg', 'SocCohAvg', \n",
    "                   'CogTrust', 'AffTrust', 'MonitoringRC', 'PsychSafetyAvg']\n",
    "# Assuming your data is structured with columns: Session ID, participant_ID, Round, and the survey measures\n",
    "\n",
    "# Iterate through each Session ID\n",
    "for measure in survey_measures:\n",
    "    for session_id, session_data in df.groupby('Session_ID'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Filter data for the current session and the rounds 'weights' and 'makecode'\n",
    "        session_data = session_data[session_data['Round'].isin(['weights', 'makecode'])]\n",
    "\n",
    "        # Get unique participant IDs\n",
    "        participant_ids = session_data['Participant_ID'].unique()\n",
    "\n",
    "        # Create a list to hold the measure values for each participant in 'weights' and 'makecode'\n",
    "        values_weights = []\n",
    "        values_makecode = []\n",
    "\n",
    "        # Iterate through each participant_ID within the Session ID\n",
    "        for participant_id in participant_ids:\n",
    "            participant_data_weights = session_data[(session_data['Participant_ID'] == participant_id) & (session_data['Round'] == 'weights')]\n",
    "            participant_data_makecode = session_data[(session_data['Participant_ID'] == participant_id) & (session_data['Round'] == 'makecode')]\n",
    "            values_weights.append(participant_data_weights[measure].values[0])\n",
    "            values_makecode.append(participant_data_makecode[measure].values[0])\n",
    "\n",
    "        # Set up positions for the bars\n",
    "        positions = range(len(participant_ids))\n",
    "        width = 0.35\n",
    "\n",
    "        # Create the bars\n",
    "        plt.bar(positions, values_weights, width, label='weights')\n",
    "        plt.bar([pos + width for pos in positions], values_makecode, width, label='makecode')\n",
    "\n",
    "        plt.title(f'{measure} for Session {session_id}')\n",
    "        plt.xlabel('Participant ID')\n",
    "        plt.ylabel(measure)\n",
    "        plt.xticks([pos + width/2 for pos in positions], participant_ids)\n",
    "        plt.legend(title='Round')\n",
    "        \n",
    "        # output_file = os.path.join(output_folder, f'{session_id}_{measure}.png')\n",
    "        # plt.savefig(output_file)\n",
    "        # plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by Session_ID, participant_ID, and Round, and calculate the mean for each measure\n",
    "grouped_data = df_filtered.groupby(['Session_ID', 'Participant_ID','Round'])\n",
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\n",
    "# Iterate through each session and round\n",
    "for (session_id, participant_id), session_data in grouped_data:\n",
    "    # Extract the values for round 2 and round 4\n",
    "    round_2_value = session_data[session_data['Round'] == 2].iloc[0]\n",
    "    round_4_value = session_data[session_data['Round'] == 4].iloc[0]\n",
    "\n",
    "    # Create a grouped bar chart for each participant\n",
    "    bar_width = 0.35\n",
    "    index = session_id\n",
    "    bar_index = [index - bar_width/2, index + bar_width/2]\n",
    "    plt.bar(bar_index, [round_2_value, round_4_value], bar_width, label=f'Participant {participant_id}')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Session ID')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Values per Participant per Session ID (Rounds 2 and 4)')\n",
    "plt.xticks(grouped_data['Session_ID'].unique())\n",
    "plt.legend(title='Participant ID')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Function to calculate SBERT embeddings for a given text\n",
    "def embed_text_sbert(text):\n",
    "    embedding = sbert_model.encode([text])\n",
    "    return embedding[0]\n",
    "\n",
    "# Apply SBERT embeddings to the 'Text' column\n",
    "df['Text_Embedding_SBERT'] = df['Text'].apply(embed_text_sbert)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between consecutive pairs of rows using SBERT embeddings\n",
    "num_embeddings = len(df['Text_Embedding_SBERT'])\n",
    "cosine_similarities = []\n",
    "\n",
    "for i in range(num_embeddings - 1):\n",
    "    # Compare embedding i with embedding i+1\n",
    "    j = i + 1 \n",
    "    # Calculate cosine similarity between embeddings i and j\n",
    "    similarity = cosine_similarity([df['Text_Embedding_SBERT'][i]], [df['Text_Embedding_SBERT'][j]])[0][0]\n",
    "    cosine_similarities.append(similarity)\n",
    "\n",
    "# Add the cosine similarities to the DataFrame\n",
    "df['Cosine_Similarity_SBERT'] = [None] + cosine_similarities\n",
    "\n",
    "# Display the resulting DataFrame with the added 'Cosine_Similarity_SBER' column\n",
    "print(df[['Speaker', 'Start', 'End', 'Text', 'Cosine_Similarity_SBERT']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove embedding columns for memory\n",
    "columns_to_drop = ['Text_Embedding_USE', 'Text_Embedding_SBERT']\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embedding Models + Coherence Functions (embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embedding for text from bert model\n",
    "# bert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "# def get_embedding_bert(text):\n",
    "#     text = text.replace(\"\\n\", \" \")\n",
    "#     embedding = bert_model.encode(text)\n",
    "#     return list(embedding) \n",
    "\n",
    "# use_module_url =\"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "# use_model = hub.load(use_module_url)\n",
    "# def get_embedding_USE(text):\n",
    "#     embedding = use_model(text)\n",
    "#     return embedding\n",
    "# USE model\n",
    "# use_module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" \n",
    "# use_model = hub.load(use_module_url)\n",
    "use_model = hub.load(\"https://kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/1\") #this one works \n",
    "# use_model = hub.load(\"https://kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\") \n",
    "\n",
    "# # BERT model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "bert_model.eval()\n",
    "\n",
    "def get_inter_window_cosine(text1, text2, embedding_type):\n",
    "    \n",
    "    e1 = embed(text1, embedding_type+'_inter')\n",
    "    e2 = embed(text2, embedding_type+'_inter')\n",
    "    if embedding_type == 'USE' or embedding_type == 'ELMo' or embedding_type == 'BERT':\n",
    "        return 1 - spatial.distance.cosine(e1, e2)\n",
    "\n",
    "    else:\n",
    "        if e1 and e2:\n",
    "            return 1 - spatial.distance.cosine(e1, e2)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def embed(text, embedding_type):\n",
    "    '''\n",
    "    returns a single embedding (inter) or a list of embeddings (intra)\n",
    "    for a specified string and embedding_type\n",
    "    '''\n",
    "    \n",
    "    # universal sentence encoder\n",
    "    if embedding_type == 'USE_inter':\n",
    "        return use_model([text])\n",
    "    \n",
    "\n",
    "    # BERT interwindow\n",
    "    elif embedding_type == 'BERT_inter':\n",
    "        # interwindow\n",
    "        tokenized_text = bert_tokenizer.encode(text)\n",
    "        # convert indexed tokens in a PyTorch tensor\n",
    "        input_ids = torch.tensor(tokenized_text).unsqueeze(0)\n",
    "        # run the input tensor through the BertModel\n",
    "        # see text in above cell for what is contained in outputs variable\n",
    "        outputs = bert_model(input_ids)\n",
    "        # get the last_hidden_state\n",
    "        last_hidden_state = outputs[0]\n",
    "        # last hidden state is dimension (batch_size, sequence_length, hidden_size)\n",
    "        # we have one batch so grab this single batch - this_batch is a tensor for each token in tokenized_text\n",
    "        this_batch = last_hidden_state[0]\n",
    "        #now get the 768 dimension vector for the CLS token (the first in the list) \n",
    "        cls_vector = this_batch[0].detach().numpy()\n",
    "        return cls_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate USE embeddings for a given text\n",
    "def embed_text(text):\n",
    "    embeddings = use_embed([text])\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Apply USE embeddings to the 'Text' column\n",
    "df['Text_Embedding_USE'] = df['Text'].apply(embed_text)\n",
    "\n",
    "# Calculate cosine similarity between consecutive pairs of rows using USE embeddings\n",
    "num_embeddings = len(df['Text_Embedding_USE'])\n",
    "cosine_similarities = []\n",
    "\n",
    "for i in range(num_embeddings - 1):\n",
    "    # Compare embedding i with embedding i+1\n",
    "    j = i + 1 \n",
    "    # Calculate cosine similarity between embeddings i and j\n",
    "    similarity = cosine_similarity([df['Text_Embedding_USE'][i]], [df['Text_Embedding_USE'][j]])[0][0]\n",
    "    cosine_similarities.append(similarity)\n",
    "\n",
    "# Add the cosine similarities to the DataFrame\n",
    "df['Cosine_Similarity_USE'] = [None] + cosine_similarities\n",
    "\n",
    "# Display the resulting DataFrame with the added 'Cosine_Similarity_USE' column\n",
    "print(df[['Speaker', 'Start', 'End', 'Text', 'Cosine_Similarity_USE']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # # Apply embedding function\n",
    "    # df['bert_embedding'] = df['Text'].apply(lambda x: get_embedding_bert(x))\n",
    "    # #####ADD USE HERE \n",
    "    # # df['USE_embedding'] = df['Text'].apply(lambda x: embed(x,'USE_intra'))\n",
    "    # output_file_path = os.path.join(output_folder, f'embed_{file_name}')\n",
    "    # df.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now embed + calculate interwindow coherence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_model(['yo my name is emily'])\n",
    "embedding='USE'\n",
    "# df.Text[1]\n",
    "x=use_model([df.Text[1]])\n",
    "y=use_model([df.Text[2]]) \n",
    "print(x)\n",
    "##issue = use model results in non 1D arrays \n",
    "# spatial.distance.cosine(x,y)\n",
    "# res= get_inter_window_cosine(x,y, embedding)\n",
    "\n",
    "# e1 = embed(text1, embedding+'_inter')\n",
    "# e2 = embed(text2, embedding+'_inter')\n",
    "# if embedding_type == 'USE' or embedding_type == 'ELMo' or embedding_type == 'BERT':\n",
    "#     return 1 - spatial.distance.cosine(e1, e2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pick Weights or Makecode\n",
    "\n",
    "output_folder='/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Cleaned Transcripts/Weights/Clean'\n",
    "# output_folder='/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Cleaned Transcripts/Makecode/Clean'\n",
    "\n",
    "file_names = [file for file in os.listdir(output_folder) if file.endswith('.xlsx')]\n",
    "embedding='USE' ##BERT OR USE \n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(output_folder, file_name)\n",
    "    df = pd.read_excel(file_path)\n",
    "    num_utterances=len(df)\n",
    "    cosines=[]\n",
    "    for i in range(num_utterances-1):\n",
    "        res= get_inter_window_cosine(df.Text[i], df.Text[i+1], embedding)\n",
    "        if res:\n",
    "            cosines.append(res)\n",
    "    cosines.insert(num_utterances,np.nan)\n",
    "    df[f'coherence_{embedding}_interwindow'] = cosines\n",
    "    folder = '/Users/emilydoherty/Desktop/test'\n",
    "    output_file_path = os.path.join(folder, f'{embedding}_coherence_{file_name}')\n",
    "    df.to_excel(output_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize lists to store all cosine distances and labels for legend\n",
    "# all_cosine_distances = []\n",
    "# all_cosine_similarities=[]\n",
    "# legend_labels = []\n",
    "    # Assuming 'bert_embedding' column contains BERT embeddings as a list of lists\n",
    "    embeddings = df['bert_embedding'].tolist()\n",
    "    # Assuming 'bert_embedding' is the column containing string representations of lists\n",
    "    embeddings = df['bert_embedding'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x) \n",
    "\n",
    "    # Calculate cosine similarity between embeddings\n",
    "    num_embeddings = len(embeddings)\n",
    "    cosine_distances = []\n",
    "    cosine_sim=[]\n",
    "\n",
    "    for i in range(num_embeddings - 1):\n",
    "        # Compare embedding i with embedding i+1\n",
    "        j = i + 1 \n",
    "        # Calculate cosine distance between embeddings i and j\n",
    "        distance = cosine(embeddings[i], embeddings[j])\n",
    "        cosine_distances.append(distance)\n",
    "        similarity = cos_sim(embeddings[i], embeddings[j])\n",
    "        cosine_sim.append(similarity)\n",
    "\n",
    "   #all_cosine_distances: rows= group \n",
    "    all_cosine_distances.append(cosine_distances)\n",
    "    all_cosine_similarities.append(cosine_sim)\n",
    "    legend_labels.append(file_name)\n",
    "\n",
    "# Print the legend labels and cosine distances for troubleshooting\n",
    "print(\"Legend Labels:\", legend_labels)\n",
    "print(\"Cosine Distances:\", all_cosine_distances)\n",
    "print(\"Cosine Similarities:\", all_cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the width of the plot\n",
    "plt.figure(figsize=(15, 6))  # Adjust the width (15 inches in this example)\n",
    "\n",
    "# Plot the cosine distances for all files on one plot with legends\n",
    "for distances, label in zip(all_cosine_distances, legend_labels):\n",
    "    plt.plot(distances, label=label)\n",
    "\n",
    "plt.xlabel('Pair Index')\n",
    "plt.ylabel('Cosine Distance')\n",
    "plt.title('Cosine Distances between Consecutive Embeddings (All Files)')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), fancybox=True, shadow=True, ncol=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the width of the plot\n",
    "plt.figure(figsize=(15, 6))  # Adjust the width (15 inches in this example)\n",
    "\n",
    "# Plot the cosine distances for all files on one plot with legends\n",
    "for similarity, label in zip(all_cosine_similarities, legend_labels):\n",
    "    plt.plot(similarity, label=label)\n",
    "\n",
    "plt.xlabel('Pair Index')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.title('Cosine Similarities between Consecutive Embeddings (All Files)')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), fancybox=True, shadow=True, ncol=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_cosine_distances) - 1):\n",
    "# Calculate the average of each row\n",
    "    averages = [sum(i) / len(i) for i in all_cosine_distances]\n",
    "    sim_avg=[sum(i) / len(i) for i in all_cosine_similarities]\n",
    "\n",
    "# Create a DataFrame with the averages\n",
    "def extract_numbers(file_name):\n",
    "    numbers = re.findall(r'\\d+', file_name)\n",
    "    return ''.join(numbers)\n",
    "\n",
    "# Extracting numbers from each file name\n",
    "numbers_list = [extract_numbers(file) for file in file_names]\n",
    "numbers_list = [int(number) for number in numbers_list]\n",
    "averages_table = pd.DataFrame({'Row': range(1, len(averages) + 1), 'Group':numbers_list, 'Cos_Distance_Avg': averages, 'Cos_Sim_Avg':sim_avg})\n",
    "\n",
    "\n",
    "# Display the table\n",
    "print(averages_table)\n",
    "# averages_table.to_csv(r\"/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Other Measures/UtteranceCount_Makecode.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(averages_table['Group'], averages_table['Cos_Distance_Avg'])\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('Cosine Distance Average')\n",
    "plt.title('Cosine Distance Average by Group (weights)')\n",
    "plt.xticks(averages_table['Group'], rotation=45) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-Window Coherence via Chelsea's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for \n",
    "get_inter_window_cosine(SENTENCE_i, SENTENCE_i+1, embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = 'USE' # iteratively swap out for DCP, GloVe, ELMo, W2V, USE, BERT\n",
    "\n",
    "for window in [3, 4, 5, 6, 7, 8, 'sentence']:\n",
    "    \n",
    "    print(embedding, window)\n",
    "\n",
    "    mean_coherence = []\n",
    "    std_coherence = []\n",
    "    min_coherence = []\n",
    "    max_coherence = []\n",
    "    slope_coherence = []\n",
    "\n",
    "    for transcript in free_speech.TRANSCRIPT:\n",
    "        \n",
    "        ## CAN OPTIONALLY REMOVE STOP WORDS HERE ##\n",
    "        \n",
    "        ## ALSO I DO TEXT PREPROCESSING HERE! ##\n",
    "        ## remove all puncutation for windows in [3,8] BUT\n",
    "        ## leave end of sentence markers for the sentence-windows         \n",
    "        \n",
    "        if window == 'sentence':\n",
    "            n_grams = sent_tokenize(all_utterances)\n",
    "            cosines = []\n",
    "            for i in range(len(n_grams)-1):\n",
    "                res = get_inter_window_cosine(n_grams[i], n_grams[i+1], embedding)\n",
    "                if res:\n",
    "                    cosines.append(res)  \n",
    "                    \n",
    "        else:\n",
    "            n_grams = get_ngrams(all_utterances, window)\n",
    "            cosines = []\n",
    "            for i in range(len(n_grams)-window):\n",
    "                res = get_inter_window_cosine(n_grams[i], n_grams[i+window], embedding)\n",
    "                if res:\n",
    "                    cosines.append(res)  \n",
    "                    \n",
    "        if len(cosines) == 0:\n",
    "            mean_coherence.append(np.nan)\n",
    "            std_coherence.append(np.nan)\n",
    "            min_coherence.append(np.nan)\n",
    "            max_coherence.append(np.nan)\n",
    "        else:\n",
    "            mean_coherence.append(np.array(cosines).mean())\n",
    "            std_coherence.append(np.array(cosines).std())\n",
    "            min_coherence.append(min(cosines))\n",
    "            max_coherence.append(max(cosines))\n",
    "        \n",
    "        if window == 'sentence':\n",
    "            slope_cosines = []\n",
    "            for i in range(len(n_grams)-1):\n",
    "                # compare each window to the first window and calculate the slope\n",
    "                res = get_inter_window_cosine(n_grams[0], n_grams[i+1], embedding)\n",
    "                if res:\n",
    "                    slope_cosines.append(res)  \n",
    "            if len(slope_cosines) == 0:\n",
    "                slope_coherence.append(np.nan)\n",
    "            else:\n",
    "                slope_coherence.append(get_slope(slope_cosines))\n",
    "\n",
    "        else:\n",
    "            slope_cosines = []\n",
    "            for i in range(len(n_grams)-window):\n",
    "                # compare each window to the first window and calculate the slope\n",
    "                res = get_inter_window_cosine(n_grams[0], n_grams[i+window], embedding)\n",
    "                if res:\n",
    "                    slope_cosines.append(res)  \n",
    "            slope_coherence.append(get_slope(slope_cosines))\n",
    "\n",
    "    free_speech[f'mean_coherence_{window}_{embedding}_interwindow'] = mean_coherence\n",
    "    free_speech[f'std_coherence_{window}_{embedding}_interwindow'] = std_coherence\n",
    "    free_speech[f'min_coherence_{window}_{embedding}_interwindow'] = min_coherence\n",
    "    free_speech[f'max_coherence_{window}_{embedding}_interwindow'] = max_coherence\n",
    "    free_speech[f'tangentiality_{window}_{embedding}_interwindow'] = slope_coherence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utterance counts per speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the combined DataFrame to accumulate utterance counts\n",
    "combined_utterance_counts = pd.DataFrame(columns=['Speaker', 'Utterances', 'File'])\n",
    "\n",
    "# Define a function to calculate the number of utterances per speaker and store in the combined DataFrame\n",
    "def count_utterances_per_speaker(file_path, file_name):\n",
    "    global combined_utterance_counts  # Use the global variable\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Calculate the number of utterances per speaker\n",
    "    utterances_per_speaker = df['Speaker'].value_counts().reset_index()\n",
    "    utterances_per_speaker.columns = ['Speaker', 'Utterances']\n",
    "    \n",
    "    # Add a column for the file name\n",
    "    temp = re.findall(r'\\d+', file_name)\n",
    "    res = list(map(int, temp))    \n",
    "    oldname= str(res).replace('[','').replace(']','')\n",
    "    utterances_per_speaker['File']=int(oldname)\n",
    "    # Append the counts to the combined DataFrame\n",
    "    combined_utterance_counts = pd.concat([combined_utterance_counts, utterances_per_speaker], ignore_index=True)\n",
    "    \n",
    "    return combined_utterance_counts\n",
    "\n",
    "# Assuming file_names contain the list of filenames\n",
    "folder_path = output_folder\n",
    "file_names = [file for file in os.listdir(folder_path) if file.endswith('.xlsx')]\n",
    "\n",
    "# Iterate through each file, calculate utterances per speaker, and accumulate the counts\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    combined_utterance_counts = count_utterances_per_speaker(file_path, file_name)\n",
    "\n",
    "# Display the combined table of speakers and utterance counts across all files\n",
    "print(\"Combined Utterances per Speaker:\")\n",
    "print(combined_utterance_counts)\n",
    "\n",
    "# combined_utterance_counts.to_csv(r\"/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Other Measures/UtteranceCount_Makecode.csv\", index=False)\n",
    "# combined_utterance_counts.to_csv(r\"/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Other Measures/UtteranceCount_Weights.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Cos_Distance_Avg'])\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('Cosine Distance Average')\n",
    "plt.title('Cosine Distance Average by Group')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability if needed\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute utterance counts per speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the number of utterances per speaker and plot pie charts\n",
    "def count_utterances_per_speaker_and_plot(file_path):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Calculate the number of utterances per speaker\n",
    "    utterances_per_speaker = df['Speaker'].value_counts().reset_index()\n",
    "    utterances_per_speaker.columns = ['Speaker', 'Utterances']\n",
    "    print(\"Utterances per Speaker:\")\n",
    "    print(utterances_per_speaker)\n",
    "\n",
    "# Assuming file_names contain the list of filenames\n",
    "folder_path = output_folder\n",
    "file_names = [file for file in os.listdir(folder_path) if file.endswith('.xlsx')]\n",
    "\n",
    "# Iterate through each file and plot pie chart for utterances per speaker\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    print(f\"File: {file_name}\")\n",
    "    count_utterances_per_speaker_and_plot(file_path)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the combined DataFrame to accumulate utterance counts\n",
    "combined_utterance_counts = pd.DataFrame(columns=['Speaker', 'Utterances', 'File'])\n",
    "\n",
    "# Define a function to calculate the number of utterances per speaker and store in the combined DataFrame\n",
    "def count_utterances_per_speaker(file_path, file_name):\n",
    "    global combined_utterance_counts  # Use the global variable\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Calculate the number of utterances per speaker\n",
    "    utterances_per_speaker = df['Speaker'].value_counts().reset_index()\n",
    "    utterances_per_speaker.columns = ['Speaker', 'Utterances']\n",
    "    \n",
    "    # Add a column for the file name\n",
    "    temp = re.findall(r'\\d+', file_name)\n",
    "    res = list(map(int, temp))    \n",
    "    oldname= str(res).replace('[','').replace(']','')\n",
    "    utterances_per_speaker['File']=int(oldname)\n",
    "    # Append the counts to the combined DataFrame\n",
    "    combined_utterance_counts = pd.concat([combined_utterance_counts, utterances_per_speaker], ignore_index=True)\n",
    "    \n",
    "    return combined_utterance_counts\n",
    "\n",
    "# Assuming file_names contain the list of filenames\n",
    "folder_path = output_folder\n",
    "file_names = [file for file in os.listdir(folder_path) if file.endswith('.xlsx')]\n",
    "\n",
    "# Iterate through each file, calculate utterances per speaker, and accumulate the counts\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    combined_utterance_counts = count_utterances_per_speaker(file_path, file_name)\n",
    "\n",
    "# Display the combined table of speakers and utterance counts across all files\n",
    "print(\"Combined Utterances per Speaker:\")\n",
    "print(combined_utterance_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add utterance count to main sheet, and cosine averages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file ='/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Other Measures/AICLsurvey_long.xlsx'\n",
    "all = pd.read_excel(file)\n",
    "# X=pd.merge(all,combined_utterance_counts,how='left',left_on='Participant_ID',right_on='Speaker').drop(['Speaker'], axis=1).drop(['File'], axis=1)\n",
    "X=pd.merge(all,averages_table,how='left',left_on='Session_ID',right_on='Group').drop(['Group'], axis=1).drop(['Row'], axis=1)\n",
    "\n",
    "X.to_excel('/Users/emilydoherty/Library/CloudStorage/OneDrive-UCB-O365/Emily_Papers/iSAT_discoursepaper2023/Other Measures/AICLsurvey_long.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
